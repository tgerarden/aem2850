---
#########################################################
# Steps to complete this homework:
#   1. Add your name to the author field of the yaml header
#   2. Fill in the code chunks and use inline code to answer the questions 
#   3. Click on "Render" directly above to render output (or Ctrl/Cmd-Shift-K)
#   4. Repeat steps 2-3 until you are satisfied with the final product
#   5. Download the resulting file homework-xx.pdf
#   6. Upload homework-xx.pdf to canvas
# Reminder: to work interactively, you can run code chunks on their own
# You can do this using keyboard shortcuts, icons in each chunk, or Run at the top right of this pane
#########################################################
title: "Homework - Week 12"
author: "your name here"
date: today
format: pdf
urlcolor: blue
---

```{r setup, include = FALSE}
library(tidyverse)
library(tidytext)
library(RColorBrewer)
library(wordcloud)

# use set.seed to ensure word cloud is reproducible
set.seed(2850)

# center plots by default
library(knitr)
knitr::opts_chunk$set(fig.align = "center",
                      out.width = "75%")
theme_set(theme_bw())
```

# Preface

The goal of this assignment is to help you gain more familiarity with processing text data. As always, please come to office hours and reach out to your teaching staff if you have any questions.

## Data

We will work with data on data scientist job postings in the U.S. scraped from popular job boards by [JobSpikr](https://www.kaggle.com/datasets/jobspikr/data-scientist-job-postings-from-the-usa?select=data_scientist_united_states_job_postings_jobspikr.csv).

```{r message = FALSE}
job_posts <- read_csv("data_scientist_united_states_job_postings.csv") |> 
  select(-cursor, -contains("contact"), -uniq_id, -html_job_description) |> 
  relocate(crawl_timestamp, url, .after = last_col())
job_posts |> 
  head(5)
```

{{< pagebreak >}}
## 1. Let's start by looking at the job title. We see that all of the job titles for the first few entries include "data scientist." Tokenize `job_title` to bigrams (i.e., n-grams with n = 2). Use a bar chart to show the top ten bigrams that appear in `job_title`, in order from most to least frequent.
<!-- *Note: To avoid case sensitivity, `unnest_tokens` converts text to all lower cases by default.* -->

```{r out.width = "65%"}

```
\vfill
## What are the most common bigrams? Do they make sense to you?
...


{{< pagebreak >}}
## 2. Let's look at the category of the jobs in `job_posts`. Tokenize `category` into individual words and remove the stop words contained in `stop_words`. Assign the tokens to the variable `word`, then `count` up the unique instances of `word`, and assign the resulting data frame to the name `category_count`. Then use a bar chart to show the top 10 most common words, in order from most to least frequent.
<!-- *Note: `stop_words` is a data frame from the `tidytext` package, which was already loaded above.* -->

```{r}

```


{{< pagebreak >}}
## 3. Let's try using a word cloud to visualize the tokens we extracted from `category`. Since this is not something we covered in class, we have provided the code for you below. You just need to remove `eval = FALSE` from the top of the code chunk, so it says `{r out.width = "50%"}` rather than `{r out.width = "50%", eval = FALSE}`.
<!-- *Tip: If the code does not work as expected, confirm that the data frame and variable names you created from question 2 match the instructions and the object names in the code below.* -->
<!-- *Note: We are using `wordcloud()` from the package `wordcloud` to make a word cloud plot. Remember that you can type ?wordcloud to get the help file to understand how to use a new function. Also note that `wordcloud()` does not take data frames as an argument. We are getting around this by using `pull()` as a way to get each of the vectors of words from inside the data frame `category_count`.* -->

```{r out.width = "50%", eval = FALSE}
wordcloud(
  words = category_count |> pull(word), # make sure category_count, word
  freq = category_count |> pull(n),     #   and n are in your data frame
  scale = c(3, 0.5),
  min.freq = 5,
  max.words = 100, 
  random.order = FALSE, 
  rot.per = 0.30, 
  colors = brewer.pal(8, "Dark2")
)
```
\vfill
## Comment on the word cloud: Is it easy to digest?
...


{{< pagebreak >}}
## 4. Where are these jobs located? Use a bar chart to show the number of job postings of the top 10 cities. 
<!-- *Note: For this, you can just take the contents of city at face value, you do not need to modify it.* -->

```{r}

```


{{< pagebreak >}}
## 5. What software skills are most commonly required for these jobs? To find out, create logical variables to indicate whether each `job_description` contains the following skill requirements: R, python, tableau, java, and sql. Then calculate the share of postings that require each of these skills, and show them in a bar plot where the y axis ranges from a share of 0 (no listings) to 1 (all listings).
<!-- *Note: This is a refresher on some of the content we learned in Week 5. As you know, you can use `str_detect()` to detect whether `job_descriptions` contain each skill (though you may have to get creative for detecting R without too many false positives!). You should also either start by using `str_to_lower()` to convert job titles to lowercase, or use `regex(., ignore_case = TRUE)` within `str_detect()` to avoid issues with cases.* -->
<!-- *Tip: Here is guidance for one way (but not the only way) to approach this problem: Use a single call to `transmute()` to create a series of columns, one for each skill, that contain `TRUE` or `FALSE` based on the output of `str_detect()`. Then, to compute the share of postings the require each skill, `summarize_all(mean)` may come in handy. Finally, it might help to pivot that data to a tidy format. * -->

```{r}

```

## Do your results make sense? If not, can you improve them?
...
